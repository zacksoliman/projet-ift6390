{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, re\n",
    "import nltk\n",
    "from sklearn.utils import shuffle\n",
    "import os.path\n",
    "from keras.utils.visualize_util import plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tweet_analysis.Tools import data_processing as dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.regularizers import l1l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_type = ['raw_tweet', 'transformed', 'lemmatized']\n",
    "\n",
    "#Getting pandas dataframes\n",
    "airlines_data = []\n",
    "pol_data = []\n",
    "\n",
    "pol_data.append(dp.get_clean_data(lemmatize=False))\n",
    "airlines_data.append(dp.get_clean_data('Tweets', lemmatize=False))\n",
    "\n",
    "pol_data.append(dp.get_clean_data(lemmatize=True))\n",
    "airlines_data.append(dp.get_clean_data('Tweets', lemmatize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "airline_tweets = []\n",
    "pol_tweets = []\n",
    "\n",
    "pol_sentiments = []\n",
    "airline_sentiments = []\n",
    "\n",
    "# First the tweets without any tansformations\n",
    "airline_tweets.append(pol_data[0]['text'].values)\n",
    "pol_tweets.append(airlines_data[0]['text'].values)\n",
    "\n",
    "pol_sentiments.append(to_categorical(pd.Categorical.from_array(pol_data[0]['sentiment'].values).codes))\n",
    "airline_sentiments.append(to_categorical(pd.Categorical.from_array(airlines_data[0]['airline_sentiment'].values).codes))\n",
    "\n",
    "for airlines_df, pol_df in zip(airlines_data, pol_data):\n",
    "    airline_tweets.append(airlines_df['processed_text'].values)\n",
    "    pol_tweets.append(pol_df['processed_text'].values)\n",
    "    \n",
    "    pol_sentiments.append(to_categorical(pd.Categorical.from_array(pol_df['sentiment'].values).codes))\n",
    "    airline_sentiments.append(to_categorical(pd.Categorical.from_array(airlines_df['airline_sentiment'].values).codes))\n",
    "    \n",
    "del pol_data[:]\n",
    "del airlines_data[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data we want to experiment with\n",
    "\n",
    "tweets = airline_tweets[1]\n",
    "sentiment = airline_sentiments[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "n_train = (tweets.shape[0]*2) // 3\n",
    "inds = [i for i in range(tweets.shape[0])]\n",
    "np.random.shuffle(inds)\n",
    "train_inds = inds[:n_train]\n",
    "test_inds = inds[n_train:]\n",
    "\n",
    "X_train = tweets[train_inds]\n",
    "X_test = tweets[test_inds]\n",
    "y_train = sentiment[train_inds]\n",
    "y_test = sentiment[test_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "vocab.add('OOV') # For out of vocabulary words\n",
    "# max number of timesteps (chars in our case)\n",
    "max_len = 150\n",
    "\n",
    "# learn vocab\n",
    "for tweet in X_train:\n",
    "    for c in tweet:\n",
    "        vocab.add(c)\n",
    "\n",
    "vocab = list(vocab)\n",
    "\n",
    "char_id = {ch:i for i, ch in enumerate(vocab)}\n",
    "id_char = {i:ch for i, ch in enumerate(vocab)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formating input and targets...\n"
     ]
    }
   ],
   "source": [
    "# using bool to reduce memory usage\n",
    "X = np.zeros((len(tweets), max_len, len(vocab)), dtype=np.bool)\n",
    "\n",
    "print('Formating input and targets...')\n",
    "\n",
    "# set the appropriate indices to 1 in each one-hot vector\n",
    "for i, train_example in enumerate(tweets):\n",
    "    for timestep, char in enumerate(train_example):\n",
    "        if char in char_id:\n",
    "            X[i, timestep, char_id[char]] = 1 # one hot encodings of tweet characters\n",
    "        else:\n",
    "            X[i, timestep, char_id['OOV']] = 1\n",
    "\n",
    "X_train = X[train_inds]\n",
    "X_test = X[test_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.pooling import AveragePooling1D\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import backend as T\n",
    "\n",
    "class TemporalMeanPooling(Layer):\n",
    "    \"\"\"\n",
    "This is a custom Keras layer. This pooling layer accepts the temporal\n",
    "sequence output by a recurrent layer and performs temporal pooling,\n",
    "looking at only the non-masked portion of the sequence. The pooling\n",
    "layer converts the entire variable-length hidden vector sequence\n",
    "into a single hidden vector, and then feeds its output to the Dense\n",
    "layer.\n",
    "\n",
    "input shape: (nb_samples, nb_timesteps, nb_features)\n",
    "output shape: (nb_samples, nb_features)\n",
    "\"\"\"\n",
    "def __init__(self, **kwargs):\n",
    "    super(TemporalMeanPooling, self).__init__(**kwargs)\n",
    "    self.supports_masking = True\n",
    "    self.input_spec = [InputSpec(ndim=3)]\n",
    "\n",
    "def get_output_shape_for(self, input_shape):\n",
    "    return (input_shape[0], input_shape[2])\n",
    "\n",
    "def call(self, x, mask=None): #mask: (nb_samples, nb_timesteps)\n",
    "    if mask is None:\n",
    "        mask = T.mean(T.ones_like(x), axis=-1)\n",
    "    ssum = T.sum(x,axis=-2) #(nb_samples, np_features)\n",
    "    mask = T.cast(mask,T.floatx())\n",
    "    rcnt = T.sum(mask,axis=-1,keepdims=True) #(nb_samples)\n",
    "    return ssum/rcnt\n",
    "    #return rcnt\n",
    "\n",
    "def compute_mask(self, input, mask):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Start fitting...\n"
     ]
    }
   ],
   "source": [
    "print('Building model...')\n",
    "#l1l2(l1=0.001, l2=0.001)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(15, \n",
    "               activation='tanh', \n",
    "               W_regularizer=None, \n",
    "               U_regularizer=None,\n",
    "               return_sequences=True, \n",
    "               input_shape=(max_len, len(vocab))))\n",
    "\n",
    "#model.add(LSTM(30, activation='tanh', return_sequences=False))\n",
    "#model.add(TemporalMeanPooling())\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(LSTM(512, activation='tanh', return_sequences=False))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', 'precision', 'recall'])\n",
    "plot(model, to_file='model.png', show_shapes=True)\n",
    "print('Start fitting...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9358 samples, validate on 4679 samples\n",
      "Epoch 1/100\n",
      "9358/9358 [==============================] - 9s - loss: 0.9543 - acc: 0.6423 - precision: 0.4122 - recall: 0.4119 - val_loss: 0.9040 - val_acc: 0.6369 - val_precision: 0.6369 - val_recall: 0.6369\n",
      "Epoch 2/100\n",
      "9358/9358 [==============================] - 11s - loss: 0.8910 - acc: 0.6462 - precision: 0.6462 - recall: 0.6462 - val_loss: 0.9051 - val_acc: 0.6369 - val_precision: 0.6369 - val_recall: 0.6369\n",
      "Epoch 3/100\n",
      "9358/9358 [==============================] - 10s - loss: 0.8916 - acc: 0.6462 - precision: 0.6462 - recall: 0.6462 - val_loss: 0.9034 - val_acc: 0.6369 - val_precision: 0.6369 - val_recall: 0.6369\n",
      "Epoch 4/100\n",
      "9358/9358 [==============================] - 12s - loss: 0.8917 - acc: 0.6462 - precision: 0.6462 - recall: 0.6462 - val_loss: 0.9035 - val_acc: 0.6369 - val_precision: 0.6369 - val_recall: 0.6369\n",
      "Epoch 5/100\n",
      "9358/9358 [==============================] - 11s - loss: 0.8887 - acc: 0.6462 - precision: 0.6462 - recall: 0.6460 - val_loss: 0.9023 - val_acc: 0.6367 - val_precision: 0.6366 - val_recall: 0.6358\n",
      "Epoch 6/100\n",
      "9358/9358 [==============================] - 10s - loss: 0.8629 - acc: 0.6447 - precision: 0.6823 - recall: 0.5949 - val_loss: 0.8572 - val_acc: 0.6303 - val_precision: 0.7251 - val_recall: 0.5185\n",
      "Epoch 7/100\n",
      "9358/9358 [==============================] - 10s - loss: 0.8444 - acc: 0.6455 - precision: 0.7077 - recall: 0.5705 - val_loss: 0.8565 - val_acc: 0.6444 - val_precision: 0.6883 - val_recall: 0.5811\n",
      "Epoch 8/100\n",
      "9358/9358 [==============================] - 10s - loss: 0.8402 - acc: 0.6498 - precision: 0.7084 - recall: 0.5683 - val_loss: 0.8476 - val_acc: 0.6412 - val_precision: 0.7231 - val_recall: 0.5347\n",
      "Epoch 9/100\n",
      "9358/9358 [==============================] - 10s - loss: 0.8396 - acc: 0.6427 - precision: 0.7190 - recall: 0.5583 - val_loss: 0.8746 - val_acc: 0.6369 - val_precision: 0.6415 - val_recall: 0.6328\n",
      "Epoch 10/100\n",
      "9358/9358 [==============================] - 12s - loss: 0.8445 - acc: 0.6489 - precision: 0.7083 - recall: 0.5720 - val_loss: 0.8458 - val_acc: 0.6435 - val_precision: 0.7232 - val_recall: 0.5345\n",
      "Epoch 11/100\n",
      "9358/9358 [==============================] - 12s - loss: 0.8301 - acc: 0.6572 - precision: 0.7216 - recall: 0.5703 - val_loss: 0.8363 - val_acc: 0.6454 - val_precision: 0.7314 - val_recall: 0.5356\n",
      "Epoch 12/100\n",
      "9358/9358 [==============================] - 11s - loss: 0.8169 - acc: 0.6598 - precision: 0.7291 - recall: 0.5658 - val_loss: 0.8204 - val_acc: 0.6495 - val_precision: 0.7250 - val_recall: 0.5606\n",
      "Epoch 13/100\n",
      "9358/9358 [==============================] - 11s - loss: 0.8105 - acc: 0.6558 - precision: 0.7393 - recall: 0.5561 - val_loss: 0.8165 - val_acc: 0.6499 - val_precision: 0.7463 - val_recall: 0.5328\n",
      "Epoch 14/100\n",
      "9358/9358 [==============================] - 10s - loss: 0.7990 - acc: 0.6617 - precision: 0.7491 - recall: 0.5570 - val_loss: 0.8083 - val_acc: 0.6514 - val_precision: 0.7374 - val_recall: 0.5550\n",
      "Epoch 15/100\n",
      "9358/9358 [==============================] - 11s - loss: 0.7942 - acc: 0.6594 - precision: 0.7495 - recall: 0.5579 - val_loss: 0.8067 - val_acc: 0.6516 - val_precision: 0.7369 - val_recall: 0.5580\n",
      "Epoch 16/100\n",
      "9358/9358 [==============================] - 11s - loss: 0.7902 - acc: 0.6621 - precision: 0.7551 - recall: 0.5597 - val_loss: 0.8111 - val_acc: 0.6461 - val_precision: 0.7450 - val_recall: 0.5309\n",
      "Epoch 17/100\n",
      "9358/9358 [==============================] - 10s - loss: 0.7893 - acc: 0.6655 - precision: 0.7577 - recall: 0.5581 - val_loss: 0.8060 - val_acc: 0.6531 - val_precision: 0.7449 - val_recall: 0.5478\n",
      "Epoch 18/100\n",
      "9358/9358 [==============================] - 10s - loss: 0.7889 - acc: 0.6650 - precision: 0.7523 - recall: 0.5606 - val_loss: 0.8027 - val_acc: 0.6553 - val_precision: 0.7380 - val_recall: 0.5529\n",
      "Epoch 19/100\n",
      "9358/9358 [==============================] - 12s - loss: 0.7863 - acc: 0.6657 - precision: 0.7560 - recall: 0.5602 - val_loss: 0.8023 - val_acc: 0.6551 - val_precision: 0.7379 - val_recall: 0.5518\n",
      "Epoch 20/100\n",
      "9358/9358 [==============================] - 11s - loss: 0.7840 - acc: 0.6695 - precision: 0.7571 - recall: 0.5609 - val_loss: 0.8341 - val_acc: 0.6514 - val_precision: 0.6991 - val_recall: 0.5971\n",
      "Epoch 21/100\n",
      "9358/9358 [==============================] - 10s - loss: 0.7829 - acc: 0.6715 - precision: 0.7584 - recall: 0.5611 - val_loss: 0.8028 - val_acc: 0.6557 - val_precision: 0.7418 - val_recall: 0.5476\n",
      "Epoch 22/100\n",
      "9358/9358 [==============================] - 10s - loss: 0.7777 - acc: 0.6735 - precision: 0.7609 - recall: 0.5623 - val_loss: 0.8212 - val_acc: 0.6521 - val_precision: 0.7069 - val_recall: 0.5867\n",
      "Epoch 23/100\n",
      "9358/9358 [==============================] - 10s - loss: 0.7766 - acc: 0.6735 - precision: 0.7632 - recall: 0.5628 - val_loss: 0.8132 - val_acc: 0.6412 - val_precision: 0.7840 - val_recall: 0.4802\n",
      "Epoch 24/100\n",
      "9358/9358 [==============================] - 11s - loss: 0.7767 - acc: 0.6736 - precision: 0.7622 - recall: 0.5652 - val_loss: 0.8004 - val_acc: 0.6610 - val_precision: 0.7276 - val_recall: 0.5683\n",
      "Epoch 25/100\n",
      "9358/9358 [==============================] - 11s - loss: 0.7747 - acc: 0.6785 - precision: 0.7638 - recall: 0.5659 - val_loss: 0.8026 - val_acc: 0.6531 - val_precision: 0.7380 - val_recall: 0.5531\n",
      "Epoch 26/100\n",
      "9358/9358 [==============================] - 11s - loss: 0.7753 - acc: 0.6762 - precision: 0.7595 - recall: 0.5656 - val_loss: 0.8000 - val_acc: 0.6580 - val_precision: 0.7312 - val_recall: 0.5582\n",
      "Epoch 27/100\n",
      "9358/9358 [==============================] - 11s - loss: 0.7732 - acc: 0.6760 - precision: 0.7602 - recall: 0.5632 - val_loss: 0.7980 - val_acc: 0.6527 - val_precision: 0.7452 - val_recall: 0.5352\n",
      "Epoch 28/100\n",
      "9358/9358 [==============================] - 11s - loss: 0.7714 - acc: 0.6739 - precision: 0.7602 - recall: 0.5632 - val_loss: 0.8004 - val_acc: 0.6615 - val_precision: 0.7238 - val_recall: 0.5749\n",
      "Epoch 29/100\n",
      "9358/9358 [==============================] - 11s - loss: 0.7687 - acc: 0.6777 - precision: 0.7655 - recall: 0.5660 - val_loss: 0.8026 - val_acc: 0.6557 - val_precision: 0.7291 - val_recall: 0.5642\n",
      "Epoch 30/100\n",
      "9358/9358 [==============================] - 11s - loss: 0.7699 - acc: 0.6779 - precision: 0.7645 - recall: 0.5676 - val_loss: 0.8284 - val_acc: 0.6566 - val_precision: 0.7051 - val_recall: 0.5901\n",
      "Epoch 31/100\n",
      "9358/9358 [==============================] - 11s - loss: 0.7687 - acc: 0.6776 - precision: 0.7680 - recall: 0.5619 - val_loss: 0.7958 - val_acc: 0.6602 - val_precision: 0.7384 - val_recall: 0.5461\n",
      "Epoch 32/100\n",
      "9358/9358 [==============================] - 11s - loss: 0.7683 - acc: 0.6770 - precision: 0.7631 - recall: 0.5645 - val_loss: 0.8005 - val_acc: 0.6649 - val_precision: 0.7268 - val_recall: 0.5747\n",
      "Epoch 33/100\n",
      "9358/9358 [==============================] - 11s - loss: 0.7693 - acc: 0.6784 - precision: 0.7628 - recall: 0.5632 - val_loss: 0.8045 - val_acc: 0.6608 - val_precision: 0.7216 - val_recall: 0.5817\n",
      "Epoch 34/100\n",
      "9358/9358 [==============================] - 11s - loss: 0.7694 - acc: 0.6763 - precision: 0.7604 - recall: 0.5708 - val_loss: 0.7987 - val_acc: 0.6649 - val_precision: 0.7316 - val_recall: 0.5734\n",
      "Epoch 35/100\n",
      "9358/9358 [==============================] - 11s - loss: 0.7678 - acc: 0.6776 - precision: 0.7623 - recall: 0.5765 - val_loss: 0.7951 - val_acc: 0.6587 - val_precision: 0.7319 - val_recall: 0.5717\n",
      "Epoch 36/100\n",
      "9358/9358 [==============================] - 11s - loss: 0.7640 - acc: 0.6811 - precision: 0.7602 - recall: 0.5811 - val_loss: 0.7925 - val_acc: 0.6555 - val_precision: 0.7497 - val_recall: 0.5452\n",
      "Epoch 37/100\n",
      "9358/9358 [==============================] - 11s - loss: 0.7638 - acc: 0.6810 - precision: 0.7667 - recall: 0.5820 - val_loss: 0.7964 - val_acc: 0.6634 - val_precision: 0.7253 - val_recall: 0.5743\n",
      "Epoch 38/100\n",
      " 600/9358 [>.............................] - ETA: 11s - loss: 0.7083 - acc: 0.7117 - precision: 0.7854 - recall: 0.6217"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, y_train, batch_size=100, nb_epoch=100, validation_data=(X_test, y_test), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4679/4679 [==============================] - 9s     \n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, batch_size=32, verbose=1, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.91038657776130827,\n",
       " 0.63688822398585221,\n",
       " 0.63688822398585221,\n",
       " 0.63688822398585221]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "histories = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
